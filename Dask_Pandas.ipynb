{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask and Pandas\n",
    "Pandas is a very popular and powerful framework for analyzing structured data, but its biggest limitation is that it was not designed with scalability in mind. Pandas is exceptionally well suited for handling small structured datasets and is highly optimized to perform fast and efficient operations on data stored in memory.\n",
    "\n",
    "This is where Dask’s DataFrame API comes in: by providing a wrapper around Pandas that intelligently splits huge data frames into smaller pieces and spreads them across a cluster of workers, operations on huge datasets can be completed much more quickly and robustly The different pieces of the DataFrame that Dask oversees are called partitions. Each partition is a relatively small DataFrame that can be dispatched to any worker and maintains its full lineage Using Pandas, the dataset would be loaded into memory and worked on sequentially one row at a time. Dask, on the other hand, can split the data into multiple partitions, allowing the workload to be parallelized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://drive.google.com/uc?id=1kde4l6iPTUAc9oSGvzzhXkl8lIyz94IZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Managing DataFrame partitioning\n",
    "Since partitioning can have such a significant impact on performance. For example,when reading in data using the read_csv method\n",
    "of Dask DataFrames, the default partition size is 64 MB each (this is also known as the default blocksize).\n",
    "If you desire to create a DataFrame with a specific number of partitions instead, you can specify that when creating the DataFrame by passing in the npartitions argument.\n",
    "##### Converts the Pandas DataFrame to a Dask DataFrame with defining partitions.</B>\n",
    "peopleDaskDataFrame = daskDataFrame.from_pandas(peoplePandasDataFrame,<B>npartitions=2</B>) \n",
    "\n",
    "##### map_partitions generally applies a given function to each partition.\n",
    "print(people_filtered.<B>map_partitions(len)</B>.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Dask Datatypes\n",
    "Dask uses random sampling to infer datatypes to avoid scanning the entire (potentially massive) DataFrame.\n",
    "When we are dealing with big data this approach may breakdown\n",
    "\n",
    "1 : IT may identified integer columns has object due large  number NaN value.Dask will throw an exception once it begins to work on a computation.\n",
    "\n",
    "C:\\Users\\vishal\\Anaconda3\\lib\\site-packages\\dask\\local.py:270: DtypeWarning: Columns (18,38) have mixed types. Specify dtype option on import or set low_memory=False.\n",
    "\n",
    "2: If you’re working on a computer with a 64-bit CPU and running a 64-bit OS, Python will always allocate 64 bits of memory to store an integer. The advantage of using smaller datatypes where appropriate is that you can hold more data in RAM and the CPU’s cache at one time, leading to faster, more efficient computations. This means that when creating a schema for your data, you should always choose the smallest possible datatype to hold your data.\n",
    "\n",
    "print (dsf.columns)\n",
    "print (dsf._meta.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](https://drive.google.com/uc?id=1P5UxWl8HVBruPs5HeMmkZ_xTKOu_-0UR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Limitations of Dask DataFrames\n",
    "1 : Dask DataFrames do not expose the entire Pandas API. Eventhough Dask DataFrames are made up of smaller Pandas DataFrames.\n",
    "Forexample, functions that would alter the structure of the DataFrame, such as insert and pop, are not supported because Dask DataFrames are <B>immutable.</B>\n",
    "\n",
    "2 : limitation is with relational-type operations, such as join/merge,groupby, and rolling. Although these operations are supported, they are likely to involve a lot of shuffling, making them performance bottlenecks. This can be minimized, again, either by using Dask to prepare a smaller dataset that can be dumped into Pandas, or by limiting these operations to only use the index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "Dask DataFrames consist of rows (axis 0), columns (axis 1), and an index.\n",
    "DataFrame methods tend to operate row-wise by default.\n",
    "\n",
    "Inspecting how a DataFrame is partitioned can be done by accessing the divisions attribute of a DataFrame.\n",
    "\n",
    "Filtering a DataFrame can cause an imbalance in the size of each partition. For best performance, partitions should be roughly equal in size. It’s a good practice to repartition a DataFrame using the repartition method after filtering a large amount of data.\n",
    "\n",
    "For best performance, DataFrames should be indexed by a logical column, partitioned by their index, and the index should be presorted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading data from difference Source \n",
    "\n",
    "\n",
    "<B>Relational Databases</B>\n",
    "\n",
    "Reading data from a relational database system (RDBMS) into Dask is fairly easy.\n",
    "In fact, you’re likely to find that the most tedious part of interfacing with RDBMSs is setting up and configuring your Dask\n",
    "environment to do so.\n",
    "Dask uses the SQL Alchemy library to interface with RDBMS. pyodbc library to manage your ODBC drivers.\n",
    "This means you will need to install and configure SQL Alchemy, pyodbc, and the ODBC drivers for your specific RDBMS on each machine in your cluster for Dask to work correctly.\n",
    "\n",
    "username = 'jesse'\n",
    "\n",
    "password = 'DataScienceRulez'\n",
    "\n",
    "hostname = 'localhost'\n",
    "\n",
    "database_name = 'DSAS'\n",
    "\n",
    "odbc_driver = 'ODBC+Driver+13+for+SQL+Server'\n",
    "\n",
    "connection_string = 'mssql+pyodbc://{0}:{1}@{2}/{3}?driver={4}'.\n",
    "\n",
    "format(username, password, hostname, database_name, odbc_driver)\n",
    "\n",
    "data = dsf.read_sql_table('violations', connection_string, index_col='SummonsNumber', npartitions=200)\n",
    "\n",
    "<B> Hadoop and Amazon S3 </B>\n",
    "\n",
    "HDFS and S3 are two of the most popular distributed filesystems, but they have one key difference for our purposes:\n",
    "\n",
    "HDFS is designed to allow computations to run on the same nodes that serve up data, and S3 is not.\n",
    "\n",
    "Amazon designed S3 as a web service dedicated solely to file storage and retrieval. There’s absolutely no way to execute application code on S3 servers.\n",
    "This means that when you work with datastored in S3, you will always have to transmit partitions from S3 to a Dask worker node\n",
    "in order to process it. Let’s now take a look at how we can use Dask to read data from these systems.\n",
    "\n",
    "While reading mutiple files some files may not have same columns, so take out common_columns to read all CSV in one go.\n",
    "\n",
    "\n",
    "data = dsf.read_csv('s3://my-bucket/nyc-parking-tickets/*.csv', dtype=dtypes,usecols=common_columns) \n",
    "\n",
    "Our read_csv call is (again) almost exactly the same  however, we’ve prefixed the file path with s3:// to tell Dask that the data islocated on an S3 filesystem, and my-bucket lets Dask know to look for the files in the S3 bucket associated with your AWS account named “my-bucket”. In order to use the S3 functionality, you must have the s3fs library installed on each Dask worker. The final requirement is that each Dask worker is properly configured for authenticating with S3. s3fs uses the boto library to communicate with S3.\n",
    "\n",
    "\n",
    "data = dsf.read_csv('hdfs://localhost/nyc-parking-tickets/*.csv',dtype=dtypes, usecols=common_columns)\n",
    "\n",
    "We have a read_csv call that should look very familiar by now. In fact, the only thing that’s changed is the file path.Prefixing the file path with hdfs:// tells Dask to look for the files on an HDFS cluster instead of the local filesystem, and localhost indicates that Dask should query the local HDFS NameNode for information on the whereabouts of the file. In this way, Dask makes it extremely easy to work with HDFS. The only additional requirement is that you install the hdfs3 library on each of your Dask workers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "  <tr>\n",
    "    <th colspan=\"6\">Reading data from different source </th>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>No</td>\n",
    "    <td>File System</td>\n",
    "    <td>Dask Worker Node installation</td>\n",
    "    <td colspan=\"3\" rowspan=\"4\"></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>1</td>\n",
    "    <td>Amazon S3</td>\n",
    "    <td>pip install s3fs<br>pip install boto<br></td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>2</td>\n",
    "    <td>HDFS</td>\n",
    "    <td>pip install hdfs3</td>\n",
    "  </tr>\n",
    "  <tr>\n",
    "    <td>3</td>\n",
    "    <td>RDBMS</td>\n",
    "    <td>pip install SQLAlchemy<br>pip install pyodbc<br></td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dtypes = {\n",
    "'Date First Observed': np.str,\n",
    "'Days Parking In Effect ': np.str,\n",
    "'Double Parking Violation': np.str,\n",
    "'Feet From Curb': np.float32,\n",
    "'From Hours In Effect': np.str,\n",
    "'House Number': np.str,\n",
    "'Hydrant Violation': np.str,\n",
    "'Intersecting Street': np.str,\n",
    "'Issue Date': np.str,\n",
    "'Issuer Code': np.float32,\n",
    "'Issuer Command': np.str,\n",
    "'Issuer Precinct': np.float32,\n",
    "'Issuer Squad': np.str,\n",
    "'Issuing Agency': np.str,\n",
    "'Law Section': np.float32,\n",
    "'Meter Number': np.str,\n",
    "'No Standing or Stopping Violation': np.str,\n",
    "'Plate ID': np.str,\n",
    "'Plate Type': np.str,\n",
    "'Registration State': np.str,\n",
    "'Street Code1': np.uint32,\n",
    "'Street Code2': np.uint32,\n",
    "'Street Code3': np.uint32,\n",
    "'Street Name': np.str,\n",
    "'Sub Division': np.str,\n",
    "'Summons Number': np.uint32,\n",
    "'Time First Observed': np.str,\n",
    "'To Hours In Effect': np.str,\n",
    "'Unregistered Vehicle?': np.str,\n",
    "'Vehicle Body Type': np.str,\n",
    "'Vehicle Color': np.str,\n",
    "'Vehicle Expiration Date': np.str,\n",
    "'Vehicle Make': np.str,\n",
    "'Vehicle Year': np.float32,\n",
    "'Violation Code': np.uint16,\n",
    "'Violation County': np.str,\n",
    "'Violation Description': np.str,\n",
    "'Violation In Front Of Or Opposite': np.str,\n",
    "'Violation Legal Code': np.str,\n",
    "'Violation Location': np.str,\n",
    "'Violation Post Code': np.str,\n",
    "'Violation Precinct': np.float32,\n",
    "'Violation Time': np.str\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number ofpartitions= 33\n"
     ]
    }
   ],
   "source": [
    "# Reading data set with appx 2GB in szie and 12 lakhs records \n",
    "import dask.dataframe as dd\n",
    "import os \n",
    "from dask.diagnostics import ProgressBar \n",
    "df = dd.read_csv(r'C:\\Users\\vishal\\Downloads\\nyc-parking-tickets\\*2017.csv',dtype=dtypes)\n",
    "print(\"Number ofpartitions=\",df.npartitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  1min  8.9s\n",
      "0     330844\n",
      "1     330801\n",
      "2     330892\n",
      "3     330817\n",
      "4     330888\n",
      "5     330813\n",
      "6     330866\n",
      "7     330809\n",
      "8     330815\n",
      "9     330815\n",
      "10    330883\n",
      "11    330830\n",
      "12    330877\n",
      "13    330785\n",
      "14    330804\n",
      "15    330818\n",
      "16    330776\n",
      "17    330774\n",
      "18    330855\n",
      "19    330867\n",
      "20    330820\n",
      "21    330836\n",
      "22    330825\n",
      "23    330828\n",
      "24    330809\n",
      "25    330859\n",
      "26    330835\n",
      "27    330817\n",
      "28    330839\n",
      "29    330921\n",
      "30    330844\n",
      "31    336771\n",
      "32    210395\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print the count for each partitions \n",
    "with ProgressBar():\n",
    "    print(df.map_partitions(len).compute(num_workers=8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  1min 20.1s\n",
      "['Time First Observed', 'No Standing or Stopping Violation', 'Hydrant Violation', 'Double Parking Violation']\n"
     ]
    }
   ],
   "source": [
    "#Dropping columns with more then 90 % missing values \n",
    "with ProgressBar():\n",
    "    missing_values = df.isnull().sum()\n",
    "    percent_missing = ((missing_values / df.index.size) * 100).compute(num_workers=4)\n",
    "    columns_to_drop = list(percent_missing[percent_missing >= 90].index)\n",
    "    nyc_data_clean_stage1 = df.drop(columns_to_drop, axis=1)\n",
    "    print(columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  1min 12.3s\n",
      "Before imputing vehicle color missing count= 152342\n"
     ]
    }
   ],
   "source": [
    "# Before imputing vehicle color missing count \n",
    "with ProgressBar():\n",
    "    missing_values = nyc_data_clean_stage1['Vehicle Color'].isnull().sum().compute(num_workers=4)\n",
    "    print (\"Before imputing vehicle color missing count=\",missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  1min 17.8s\n"
     ]
    }
   ],
   "source": [
    "# Imputing missing values for vehicle color categorical vairable\n",
    "with ProgressBar():\n",
    "    count_of_vehicle_colors = nyc_data_clean_stage1['Vehicle Color'].value_counts().compute()\n",
    "    most_common_color = count_of_vehicle_colors.sort_values(ascending=False).index[0]\n",
    "    nyc_data_clean_stage2 = nyc_data_clean_stage1.fillna({'Vehicle Color': most_common_color})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  1min 17.2s\n",
      "After imputing vehicle color missing count= 0\n"
     ]
    }
   ],
   "source": [
    "# After imputing vehicle color missing count value \n",
    "with ProgressBar():\n",
    "    missing_values = nyc_data_clean_stage2['Vehicle Color'].isnull().sum().compute(num_workers=4)\n",
    "    print (\"After imputing vehicle color missing count=\",missing_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed |  1min 23.0s\n",
      "Summons Number                             0\n",
      "Plate ID                                 728\n",
      "Registration State                         0\n",
      "Plate Type                                 0\n",
      "Issue Date                                 0\n",
      "Violation Code                             0\n",
      "Vehicle Body Type                      42711\n",
      "Vehicle Make                           73050\n",
      "Issuing Agency                             0\n",
      "Street Code1                               0\n",
      "Street Code2                               0\n",
      "Street Code3                               0\n",
      "Vehicle Expiration Date                    0\n",
      "Violation Location                   2072400\n",
      "Violation Precinct                         0\n",
      "Issuer Precinct                            0\n",
      "Issuer Code                                0\n",
      "Issuer Command                       2062645\n",
      "Issuer Squad                         2063541\n",
      "Violation Time                            63\n",
      "Violation County                       39547\n",
      "Violation In Front Of Or Opposite    2161235\n",
      "House Number                         2288618\n",
      "Street Name                             4009\n",
      "Intersecting Street                  7435473\n",
      "Date First Observed                        0\n",
      "Law Section                                0\n",
      "Sub Division                             773\n",
      "Violation Legal Code                 8740321\n",
      "Days Parking In Effect               2712416\n",
      "From Hours In Effect                 5450946\n",
      "To Hours In Effect                   5450943\n",
      "Vehicle Color                              0\n",
      "Unregistered Vehicle?                9675432\n",
      "Vehicle Year                               0\n",
      "Meter Number                         9017555\n",
      "Feet From Curb                             0\n",
      "Violation Post Code                  3190187\n",
      "Violation Description                1127470\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# after imputation vairfy Vehicle Color with othercolumms missing values\n",
    "with ProgressBar():\n",
    "    missing_values = nyc_data_clean_stage2.isnull().sum().compute(num_workers=4)\n",
    "    print (missing_values)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
